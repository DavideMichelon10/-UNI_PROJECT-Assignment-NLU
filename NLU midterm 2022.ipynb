{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb67ba8d",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d123d",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0d1f",
   "metadata": {},
   "source": [
    "#### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca1124f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', ',', '.', 'of', 'in', 'to', 'a', '(', ')', 'and', '\"', 'on', 'said', \"'s\", 'for', '-', '1', 'at', 'was', '2']\n",
      "['the', '.', ',', 'of', 'in', 'to', 'a', 'and', '(', ')', '\"', 'on', 'said', \"'s\", 'for', '1', '-', 'at', 'was', '2']\n",
      "['the', ',', '.', 'to', 'of', 'in', '(', ')', 'a', 'and', 'on', '\"', 'said', \"'s\", '-', 'for', 'at', 'was', '4', 'with']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from collections import Counter\n",
    "\n",
    "root = './data'\n",
    "corpus = ConllCorpusReader(root, '.txt', ('words', 'pos', 'tree'))\n",
    "whole_dataset = []\n",
    "\n",
    "test_words = [w.lower() for w in corpus.words('test.txt')]\n",
    "train_words = [w.lower() for w in corpus.words('train.txt')]\n",
    "valid_words = [w.lower() for w in corpus.words('valid.txt')]\n",
    "\n",
    "whole_dataset = test_words + train_words + valid_words\n",
    "test_plus_train = test_words + train_words\n",
    "\n",
    "test_sents = corpus.sents('test.txt')\n",
    "train_sents = corpus.sents('train.txt')\n",
    "valid_sents = corpus.sents('valid.txt')\n",
    "\n",
    "whole_dataset_freq_list = Counter(whole_dataset)\n",
    "test_plus_train_freq_list = Counter(test_plus_train)\n",
    "train_freq_list = Counter(train_words)\n",
    "test_freq_list = Counter(test_words)\n",
    "\n",
    "most_twenty_whole = [ w[0] for w in (sorted(whole_dataset_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_whole)\n",
    "\n",
    "most_twenty_train = [ w[0] for w in (sorted(train_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_train)\n",
    "\n",
    "most_twenty_test = [ w[0] for w in (sorted(test_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc02f",
   "metadata": {},
   "source": [
    "#### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(train_words) - set(test_words))\n",
    "print(set(train_words).intersection(set (test_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ac1c",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36e5c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in c:\\users\\david\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.8.9)\n",
      "13\n",
      "| Token   |   Frequence |\n",
      "|---------+-------------|\n",
      "| the     |       10155 |\n",
      "| .       |        9000 |\n",
      "| ,       |        8927 |\n",
      "| of      |        4604 |\n",
      "| in      |        4382 |\n",
      "| to      |        4229 |\n",
      "| a       |        3857 |\n",
      "| (       |        3547 |\n",
      "| )       |        3545 |\n",
      "| and     |        3470 |\n",
      "| \"       |        2599 |\n",
      "| on      |        2559 |\n",
      "| said    |        2248 |\n",
      "| 's      |        1913 |\n",
      "| for     |        1751 |\n",
      "| 1       |        1567 |\n",
      "| -       |        1530 |\n",
      "| at      |        1397 |\n",
      "| was     |        1319 |\n",
      "| 2       |        1134 |\n",
      "| 3       |        1088 |\n",
      "| 0       |        1072 |\n",
      "| with    |        1052 |\n",
      "| that    |         972 |\n",
      "| he      |         967 |\n",
      "| from    |         947 |\n",
      "| it      |         911 |\n",
      "| by      |         896 |\n",
      "| :       |         875 |\n",
      "| is      |         836 |\n",
      "| 4       |         782 |\n",
      "| as      |         753 |\n",
      "| had     |         700 |\n",
      "| his     |         682 |\n",
      "| not     |         665 |\n",
      "| but     |         664 |\n",
      "| were    |         659 |\n",
      "| an      |         650 |\n",
      "| has     |         649 |\n",
      "| be      |         641 |\n",
      "| after   |         628 |\n",
      "| have    |         613 |\n",
      "| new     |         568 |\n",
      "| first   |         562 |\n",
      "| 5       |         548 |\n",
      "| who     |         538 |\n",
      "| 6       |         503 |\n",
      "| will    |         489 |\n",
      "| two     |         471 |\n",
      "| they    |         470 |\n",
      "18671\n"
     ]
    }
   ],
   "source": [
    "! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "number_sents = len(test_sents + train_sents)\n",
    "# 1\n",
    "print(round(len(test_words + train_words)/number_sents))\n",
    "\n",
    "#2\n",
    "most_frequent_words = sorted(test_plus_train_freq_list.items(), key=lambda item: item[1] ,reverse= True)\n",
    "print(tabulate(most_frequent_words[:50], headers=['Token', 'Frequence'], tablefmt='orgtbl'))\n",
    "\n",
    "#3\n",
    "# Do we have to consider also the first empty line?\n",
    "print(number_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af097",
   "metadata": {},
   "source": [
    "#### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5659670d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Token   |   Frequence |\n",
      "|---------+-------------|\n",
      "| B-LOC   |        8808 |\n",
      "| B-PER   |        8217 |\n",
      "| B-ORG   |        7982 |\n",
      "| I-PER   |        5684 |\n",
      "| I-ORG   |        4539 |\n",
      "| B-MISC  |        4140 |\n",
      "| I-LOC   |        1414 |\n",
      "| I-MISC  |        1371 |\n",
      "| Token   |   Frequence |\n",
      "|---------+-------------|\n",
      "| B-LOC   |        7140 |\n",
      "| B-PER   |        6600 |\n",
      "| B-ORG   |        6321 |\n",
      "| I-PER   |        4528 |\n",
      "| I-ORG   |        3704 |\n",
      "| B-MISC  |        3438 |\n",
      "| I-LOC   |        1157 |\n",
      "| I-MISC  |        1155 |\n",
      "| Token   |   Frequence |\n",
      "|---------+-------------|\n",
      "| B-LOC   |        1668 |\n",
      "| B-ORG   |        1661 |\n",
      "| B-PER   |        1617 |\n",
      "| I-PER   |        1156 |\n",
      "| I-ORG   |         835 |\n",
      "| B-MISC  |         702 |\n",
      "| I-LOC   |         257 |\n",
      "| I-MISC  |         216 |\n"
     ]
    }
   ],
   "source": [
    "# frequency of each Named Entities: NN, ...\n",
    "# \n",
    "corpus = ConllCorpusReader('./data', '.txt', ('words', 'pos','tree', 'chunk'))\n",
    "\n",
    "def nbest(d):\n",
    "    return sorted(d.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "def print_most_frequent(first_file, second_file, corpus):\n",
    "    named_entity = list(filter(lambda entity: entity != 'O', [entity [2] for entity in corpus.iob_words(first_file)]))\n",
    "    if second_file is not None:\n",
    "        named_entity_second = list(filter(lambda entity: entity != 'O', [entity [2] for entity in corpus.iob_words(second_file)]))\n",
    "        named_entity = named_entity + named_entity_second\n",
    "\n",
    "    named_entity_counter = Counter(named_entity)\n",
    "    print(tabulate(nbest(named_entity_counter), headers=['Token', 'Frequence'], tablefmt='orgtbl'))\n",
    "\n",
    "print_most_frequent('train.txt', 'test.txt', corpus)\n",
    "print_most_frequent('train.txt', None, corpus)\n",
    "print_most_frequent('test.txt', None, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08f37",
   "metadata": {},
   "source": [
    "### Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40387f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba597",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84838829",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d524e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292e99ac",
   "metadata": {},
   "source": [
    "#### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b1106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3358779",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820ad69",
   "metadata": {},
   "source": [
    "#### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051c93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d669ee84",
   "metadata": {},
   "source": [
    "### Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc4f",
   "metadata": {},
   "source": [
    "#### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b182ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41ebf011",
   "metadata": {},
   "source": [
    "#### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5177f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa4657c",
   "metadata": {},
   "source": [
    "#### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b94966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
