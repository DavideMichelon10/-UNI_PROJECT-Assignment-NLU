{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb67ba8d",
   "metadata": {},
   "source": [
    "# NLU: Mid-Term Assignment 2022\n",
    "### Description\n",
    "In this notebook, we ask you to complete four main tasks to show what you have learnt during the NLU labs. Therefore, to complete the assignment please refer to the concepts, libraries and other materials shown and used during the labs. The last task is not mandatory, it is a *BONUS* to get an extra mark for the laude. \n",
    "\n",
    "### Instructions\n",
    "- **Dataset**: in this notebook, you are asked to work with the dataset *Conll 2003* provided by us in the *data* folder. Please, load the files from the *data* folder and **do not** change names or paths of the inner files. \n",
    "- **Output**: for each part of your task, print your results and leave it in the notebook. Please, **do not** send a jupyter notebook without the printed outputs.\n",
    "- **Other**: follow carefully all the further instructions and suggestions given in the question descriptions.\n",
    "\n",
    "### Deadline\n",
    "The deadline is due in two weeks from the project presentation. Please, refer to *piazza* channel for the exact date."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996d123d",
   "metadata": {},
   "source": [
    "### Task 1: Analysis of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ead0d1f",
   "metadata": {},
   "source": [
    "#### Q 1.1\n",
    "- Create the Vocabulary and Frequency Dictionary of the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set\n",
    "    \n",
    "**Attention**: print the first 20 words of the Dictionaty of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1124f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole dataset: all dataset (anche validation) - 1.1, 1.4 consider also the validation. \n",
    "# no lowercase for name entity\n",
    "\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from collections import Counter\n",
    "\n",
    "root = './data'\n",
    "corpus = ConllCorpusReader(root, '.txt', ('words', 'pos','tree', 'chunk'))\n",
    "whole_dataset = []\n",
    "\n",
    "test_words = [w.lower() for w in corpus.words('test.txt')]\n",
    "train_words = [w.lower() for w in corpus.words('train.txt')]\n",
    "valid_words = [w.lower() for w in corpus.words('valid.txt')]\n",
    "\n",
    "whole_dataset = test_words + train_words + valid_words\n",
    "test_plus_train = test_words + train_words\n",
    "\n",
    "test_sents = corpus.sents('test.txt')\n",
    "train_sents = corpus.sents('train.txt')\n",
    "\n",
    "whole_dataset_freq_list = Counter(whole_dataset)\n",
    "test_plus_train_freq_list = Counter(test_plus_train)\n",
    "train_freq_list = Counter(train_words)\n",
    "test_freq_list = Counter(test_words)\n",
    "\n",
    "most_twenty_whole = [ w[0] for w in (sorted(whole_dataset_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_whole)\n",
    "\n",
    "most_twenty_train = [ w[0] for w in (sorted(train_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_train)\n",
    "\n",
    "most_twenty_test = [ w[0] for w in (sorted(test_freq_list.items(), key=lambda item: item[1] ,reverse= True)[:20]) ]\n",
    "print(most_twenty_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0dc02f",
   "metadata": {},
   "source": [
    "#### Q 1.2\n",
    "- Obtain the list of:\n",
    "    1. Out-Of-Vocabulary (OOV) tokens\n",
    "    2. Overlapping tokens between train and test sets  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV = set(train_words) - set(test_words)\n",
    "overlap = (set(train_words)).intersection(set(test_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1ac1c",
   "metadata": {},
   "source": [
    "#### Q 1.3\n",
    "- Perform a complete data analysis of the whole dataset (train + test sets) to obtain:\n",
    "    1. Average sentence length computed in number of tokens\n",
    "    2. The 50 most-common tokens\n",
    "    3. Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "number_sents = len(test_sents + train_sents)\n",
    "# 1\n",
    "print(len(test_words + train_words)/number_sents)\n",
    "\n",
    "#2\n",
    "most_frequent_words = sorted(test_plus_train_freq_list.items(), key=lambda item: item[1] ,reverse= True)\n",
    "print(tabulate(most_frequent_words[:50], headers=['Token', 'Frequence'], tablefmt='orgtbl'))\n",
    "\n",
    "#3\n",
    "# Do we have to consider also the first empty line?\n",
    "print(number_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726af097",
   "metadata": {},
   "source": [
    "#### Q 1.4\n",
    "- Create the dictionary of Named Entities and their Frequencies for the:\n",
    "    1. Whole dataset\n",
    "    2. Train set\n",
    "    3. Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of each Named Entities: NN, ...\n",
    "# list of all words\n",
    "\n",
    "def nbest(d):\n",
    "    return sorted(d.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "def create_dictionary_NE_frequency(file, second_file = None):\n",
    "    words = corpus.iob_words(file)\n",
    "\n",
    "    if second_file is not None:\n",
    "        words_second = corpus.iob_words(second_file)\n",
    "        words = words + words_second\n",
    "    \n",
    "    names = []\n",
    "    word = []\n",
    "\n",
    "    for entity in words:\n",
    "        print(entity)\n",
    "        if \"-\" in entity[2]:\n",
    "            prefix, suffix = entity[2].split(\"-\")\n",
    "            if prefix == \"B\":\n",
    "                names.append(' '.join(word))\n",
    "                word.clear()\n",
    "                word.append(entity[0])\n",
    "            if prefix == \"I\":\n",
    "                word.append(entity[0])\n",
    "\n",
    "    return Counter(names)\n",
    "    \n",
    "\n",
    "print(tabulate(nbest(create_dictionary_NE_frequency(\"test.txt\"))[:5], headers=['Test token', 'Frequence'], tablefmt='orgtbl'))\n",
    "print()\n",
    "print(tabulate(nbest(create_dictionary_NE_frequency(\"train.txt\"))[:5], headers=['Train token', 'Frequence'], tablefmt='orgtbl'))\n",
    "print()\n",
    "print(tabulate(nbest(create_dictionary_NE_frequency(\"train.txt\", \"test.txt\"))[:5], headers=['Test and Train token', 'Frequence'], tablefmt='orgtbl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a08f37",
   "metadata": {},
   "source": [
    "### Task 2: Working with Dependecy Tree\n",
    "*Suggestions: use Spacy pipeline to retreive the Dependecy Tree*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40387f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ba597",
   "metadata": {},
   "source": [
    "#### Q 2.1\n",
    "- Given each sentence in the dataset, write the required functions to provide:\n",
    "    1. Subject, obects (direct and indirect)\n",
    "    2. Noun chunks\n",
    "    3. The head noun in each noun chunk\n",
    "    \n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lab 6 - 1.1\n",
    "# Noun chunks are “base noun phrases” – flat phrases that have a noun as their head. \n",
    "# You can think of noun chunks as a noun plus the words describing the noun\n",
    "\n",
    "sent = \"I saw the man with a telescope\"\n",
    "\n",
    "def get_subjects(doc):\n",
    "    for t in doc:\n",
    "        dep = t.dep_\n",
    "        if dep == 'nsubj': print(\"Nominal subject: '{}'\".format(t))\n",
    "        elif dep == 'dobj': print(\"Direct object: '{}'\".format(t))\n",
    "        elif dep == 'iobj': print(\"Indirect object: '{}'\".format(t))\n",
    "\n",
    "def get_noun_chunks(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        print(\"Noun chunk: '{}' \\n Head Noun: '{}'\".format(chunk.text, chunk.root.text))\n",
    "        print()\n",
    "\n",
    "get_subjects(nlp(sent))\n",
    "print()\n",
    "get_noun_chunks(nlp(sent))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84838829",
   "metadata": {},
   "source": [
    "#### Q 2.2\n",
    "- Given a dependecy tree of a sentence and a segment of that sentence write the required functions that ouput the dependency subtree of that segment.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\" (the segment could be any e.g. \"saw the man\", \"a telescope\", etc.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d524e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of tree as a spacy doc\n",
    "# subtree from spacy: <generator> object\n",
    "# in sentence there are chunks --> dependency of that chunk\n",
    "# the segment is a chunk \n",
    "\n",
    "from nltk import Tree\n",
    "import en_core_web_sm\n",
    "spacy_nlp = en_core_web_sm.load()\n",
    "\n",
    "#thanks to: https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "def output_subtree(sentence, segment):\n",
    "    words = segment.split()\n",
    "    doc = spacy_nlp(sentence)\n",
    "    not_leaf = []\n",
    "    for sent in doc.sents:\n",
    "        tree = to_nltk_tree(sent.root)\n",
    "        for sub in tree.subtrees():\n",
    "            if sub.label() in words:\n",
    "                not_leaf.append(sub.label())\n",
    "                sub.pretty_print() \n",
    "    \n",
    "    leafs = set(words) - set(not_leaf)\n",
    "    for f in leafs:\n",
    "        print(\"leaf: {}\".format(f))\n",
    "\n",
    "\n",
    "sent = \"I saw the man with a telescope\"\n",
    "output_subtree(sent, \"a telescope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e99ac",
   "metadata": {},
   "source": [
    "#### Q 2.3\n",
    "- Given a token in a sentence, write the required functions that output the dependency path from the root of the dependency tree to that given token.\n",
    "\n",
    "**Attention**: *print only the results of these functions by using the sentence \"I saw the man with a telescope\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0b1106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the path\n",
    "\n",
    "def path_to_root(doc, segment):\n",
    "    words = segment.split()\n",
    "\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    for descendant in root.subtree:\n",
    "        if descendant.text in words:\n",
    "            print(\"word: {}, path to root: {}\".format(descendant, [ancestor.text for ancestor in descendant.ancestors]))\n",
    "\n",
    "sent = \"I saw the man with a telescope\"\n",
    "\n",
    "doc = nlp(sent)\n",
    "path_to_root(doc, \"I saw the man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3358779",
   "metadata": {},
   "source": [
    "### Task 3: Named Entity Recognition\n",
    "*Suggestion: use scikit-learn metric functions. See classification_report*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820ad69",
   "metadata": {},
   "source": [
    "#### Q 3.1\n",
    "- Benchmark Spacy Named Entity Recognition model on the test set by:\n",
    "    1. Providing the list of categories in the dataset (person, organization, etc.)\n",
    "    2. Computing the overall accuracy on NER\n",
    "    3. Computing the performance of the Named Entity Recognition model for each category:\n",
    "        - Compute the perfomance at the token level (eg. B-Person, I-Person, B-Organization, I-Organization, O, etc.)\n",
    "        - Compute the performance at the entity level (eg. Person, Organization, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import conll\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./data/test.txt'))\n",
    "\n",
    "from conll import evaluate, read_corpus_conll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the accuracy as a metric\n",
    "# length of sequences are the same\n",
    "# use the IOB\n",
    "# look at conll.py on piaza, we have to use it\n",
    "# entity can consist in one or more token\n",
    "# when we evaluate a token consider it in the whole\n",
    "\n",
    "# token level: classification level\n",
    "# entity level: conll.py\n",
    "\n",
    "#spacy has different taglist. Having a mapping (name to person)\n",
    "# think a model and compare with groundtrough\n",
    "# make sure that tokenizer doesnt tokenize the sentence\n",
    "\n",
    "# LAB 7\n",
    "\n",
    "# first point\n",
    "def get_categories(refs):\n",
    "    categories = []\n",
    "    for sent in refs:\n",
    "        for iob in sent:\n",
    "            if \"-\" in iob:\n",
    "                iob = iob[2:]\n",
    "                categories.append(iob)\n",
    "    return categories\n",
    "\n",
    "refs_test = [[iob for text, pos, iob in sent] for sent in  corpus.iob_sents(\"test.txt\")]\n",
    "refs_train = [[iob for text, pos, iob in sent] for sent in  corpus.iob_sents(\"train.txt\")]\n",
    "refs_valid = [[iob for text, pos, iob in sent] for sent in  corpus.iob_sents(\"valid.txt\")]\n",
    "\n",
    "print(set(get_categories(refs_test) + get_categories(refs_train) + get_categories(refs_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "named_entity = list(filter(lambda entity: entity != 'O', [entity for entity in corpus.iob_words(\"test.txt\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "match = {\n",
    "    \"PERSON\": \"PER\",\n",
    "    \"GPE\": \"LOC\",\n",
    "    \"ORG\": \"ORG\",\n",
    "}\n",
    "\n",
    "def check_name(iob, name):\n",
    "    if iob == 'O' or name not in match:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return \"-\".join([iob, match.get(name)])\n",
    "\n",
    "# parsing test set\n",
    "res = []\n",
    "for sent in new_corpus:\n",
    "    doc = nlp(\" \".join([t[0] for t in sent]))\n",
    "    out = []\n",
    "    for t in doc:\n",
    "        out.append((t.text, check_name(t.ent_iob_, t.ent_type_)))\n",
    "    res.append(out)\n",
    "\n",
    "res1 = evaluate(new_corpus, res)\n",
    "pd_tbl = pd.DataFrame().from_dict(res1, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669ee84",
   "metadata": {},
   "source": [
    "### Task 4: BONUS PART (extra mark for laude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fc4f",
   "metadata": {},
   "source": [
    "#### Q 4.1\n",
    "- Modify NLTK Transition parser's Configuration calss to use better features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b182ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# google it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ebf011",
   "metadata": {},
   "source": [
    "#### Q 4.2\n",
    "- Evaluate the features comparing performance to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5177f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa4657c",
   "metadata": {},
   "source": [
    "#### Q 4.3\n",
    "- Replace SVM classifier with an alternative of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b94966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
